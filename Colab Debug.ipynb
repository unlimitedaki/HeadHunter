{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Colab Debug.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1Fxt_8OMJWdiN0XzGDoQyxnMbAFlKMFYj","authorship_tag":"ABX9TyP4xZHjPoUXJNl4GQvp0xqg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c386c2f3f93841caacd12588a79289ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_111c874a80ff436b91e5f7d407eee71d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fe77121a5d754c96b684bce72ddbe978","IPY_MODEL_e10b28cbada84cb78cd69c3f6e684971"]}},"111c874a80ff436b91e5f7d407eee71d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fe77121a5d754c96b684bce72ddbe978":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9459486290704ce6911d9719e9c03c00","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_039e0d579e2f487c8160300824d409c5"}},"e10b28cbada84cb78cd69c3f6e684971":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d9fc43dc8c48495eb07c8cf50e1b1c88","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 4.38kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_51f6365469dd44049671160553969111"}},"9459486290704ce6911d9719e9c03c00":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"039e0d579e2f487c8160300824d409c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d9fc43dc8c48495eb07c8cf50e1b1c88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"51f6365469dd44049671160553969111":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"996676f15ee047ba8d300bb7f713393f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_008e58e33bb34ac5b4dbbb3850fd2b92","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ce4ad924656a4cf9b3c266bc769cbd18","IPY_MODEL_edee8d4014c4435fa29da329cb30675c"]}},"008e58e33bb34ac5b4dbbb3850fd2b92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ce4ad924656a4cf9b3c266bc769cbd18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_14a0dfa7c7924189a1ce86bb9a92b1d3","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":435779157,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435779157,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_251c40a0378d431b8ee8c0d49689e258"}},"edee8d4014c4435fa29da329cb30675c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aad72d8e03f14d8eb1fbe8293077f2bf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 436M/436M [00:08&lt;00:00, 54.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_31ce00aa44394bff8304aef7304b3189"}},"14a0dfa7c7924189a1ce86bb9a92b1d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"251c40a0378d431b8ee8c0d49689e258":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aad72d8e03f14d8eb1fbe8293077f2bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"31ce00aa44394bff8304aef7304b3189":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"MBNqrtf74OsP","executionInfo":{"status":"ok","timestamp":1604320786862,"user_tz":-480,"elapsed":1129,"user":{"displayName":"快刀切草莓君","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVI52hY-Keqz35442eBgkijOY1l24OWmJ_8DTf=s64","userId":"16350139520541830195"}}},"source":["from google.colab import drive"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"9nJzC1mq4R_2","executionInfo":{"status":"ok","timestamp":1604320787259,"user_tz":-480,"elapsed":1515,"user":{"displayName":"快刀切草莓君","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVI52hY-Keqz35442eBgkijOY1l24OWmJ_8DTf=s64","userId":"16350139520541830195"}},"outputId":"5c32f978-bd5e-4f03-a528-2703dcc66eb3","colab":{"base_uri":"https://localhost:8080/"}},"source":["drive.mount('/content/drive')\n","%cd ./drive/My\\ Drive"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bmue9nkU4kNC","executionInfo":{"status":"ok","timestamp":1604320787260,"user_tz":-480,"elapsed":1511,"user":{"displayName":"快刀切草莓君","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVI52hY-Keqz35442eBgkijOY1l24OWmJ_8DTf=s64","userId":"16350139520541830195"}},"outputId":"1e1dc16d-d8b5-4fe2-a4ed-1c46ebe919b2","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd rerank_CSQA/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/rerank_CSQA\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HrUCuVng6yJ5","executionInfo":{"status":"ok","timestamp":1604321016948,"user_tz":-480,"elapsed":231198,"user":{"displayName":"快刀切草莓君","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVI52hY-Keqz35442eBgkijOY1l24OWmJ_8DTf=s64","userId":"16350139520541830195"}}},"source":["import os \n","import sys\n","import json\n","import logging\n","import argparse\n","import time\n","import pdb\n","import random\n","if os.path.exists(\"external_libraries\"):\n","    sys.path.append('external_libraries')\n","\n","import numpy as np\n","from apex import amp\n","from tqdm.notebook import tqdm\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, DistributedSampler\n","import transformers\n","from transformers import BertModel,BertTokenizer,AlbertTokenizer,RobertaTokenizer,XLNetTokenizer\n","from transformers import AdamW,get_linear_schedule_with_warmup\n","from transformers.modeling_utils import SequenceSummary\n","\n","from processor import *\n","from model import *\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"BdtnsTH42PI3","executionInfo":{"status":"ok","timestamp":1604321016953,"user_tz":-480,"elapsed":231201,"user":{"displayName":"快刀切草莓君","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVI52hY-Keqz35442eBgkijOY1l24OWmJ_8DTf=s64","userId":"16350139520541830195"}}},"source":["def select_tokenizer(args):\n","    if \"albert\" in args.origin_model:\n","        return AlbertTokenizer.from_pretrained(args.origin_model)\n","    elif \"roberta\" in args.origin_model:\n","        return RobertaTokenizer.from_pretrained(args.origin_model)\n","    elif \"bert\" in args.origin_model:\n","        return BertTokenizer.from_pretrained(args.origin_model)\n","    elif \"xlnet\" in args.origin_model:\n","        return XLNetTokenizer.from_pretrained(args.origin_model)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"tsmggxms66Bc","executionInfo":{"status":"ok","timestamp":1604321016954,"user_tz":-480,"elapsed":231201,"user":{"displayName":"快刀切草莓君","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVI52hY-Keqz35442eBgkijOY1l24OWmJ_8DTf=s64","userId":"16350139520541830195"}}},"source":["def select_model(args):\n","    cache = os.path.join(args.output_dir, \"cache\")\n","    if \"albert\" in args.origin_model:\n","        return AlbertForMultipleChoice.from_pretrained(args.origin_model, cache_dir=cache)\n","    elif \"roberta\" in args.origin_model:\n","        return RobertaForMultipleChoice.from_pretrained(args.origin_model, cache_dir=cache)\n","    elif \"bert\" in args.origin_model:\n","        return BertForMultipleChoice.from_pretrained(args.origin_model, cache_dir=cache)\n","    elif \"xlnet\" in args.origin_model:\n","        return XLNetForMultipleChoice.from_pretrained(args.origin_model, cache_dir=cache)\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcfuyCkH67Dw","executionInfo":{"status":"ok","timestamp":1604321016954,"user_tz":-480,"elapsed":231199,"user":{"displayName":"快刀切草莓君","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVI52hY-Keqz35442eBgkijOY1l24OWmJ_8DTf=s64","userId":"16350139520541830195"}}},"source":["def train(args):\n","    '''\n","    Train the model, return Nothing\n","    '''\n","    # set up output_dir\n","    output_dir = os.path.join(args.output_dir,args.save_model_name)\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    # save all the args\n","    arg_dict = args.__dict__\n","    with open(os.path.join(output_dir,\"args.json\"),'w',encoding='utf8') as f:\n","        json.dump(arg_dict,f,indent=2,ensure_ascii=False)\n","    # setup logging\n","    logfilename = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())+\" \"+args.save_model_name+\".log.txt\"\n","    fh = logging.FileHandler(os.path.join(output_dir,logfilename), mode='a', encoding='utf-8')\n","    fh.setLevel(logging.INFO)\n","    logger.addHandler(fh)\n","    # freeze seed\n","    if args.seed:\n","        set_seed(args)\n","    # loading data\n","    omcs_corpus = load_omcs(args)\n","    tokenizer = select_tokenizer(args)\n","    _,_,train_dataset= load_csqa_omcs_dataset(tokenizer,args,omcs_corpus,\"train\")\n","    dev_examples,_,dev_dataset= load_csqa_omcs_dataset(tokenizer,args,omcs_corpus,\"dev\")\n","    # _,_,test_dataset= load_csqa_omcs_dataset(tokenizer,args,omcs_corpus,\"test\",is_training = False)\n","    # setup device\n","    if args.tpu:\n","        # xla packages\n","        import torch_xla\n","        import torch_xla.distributed.data_parallel as dp\n","        import torch_xla.debug.metrics as met\n","        import torch_xla.utils.utils as xu\n","        import torch_xla.core.xla_model as xm\n","        import torch_xla.test.test_utils as test_utils\n","        # use multi thread method\n","        devices = (xm.get_xla_supported_devices(max_devices = 8))\n","        args.learning_rate = args.learning_rate * max(len(devices), 1)\n","        logger.info(\"New learning_rate for TPU is {}\".format(str(args.learning_rate)))\n","        device_num = len(devices)\n","        train_sampler = torch.utils.data.distributed.DistributedSampler(\n","            train_dataset,\n","            num_replicas = xm.xrt_world_size(),\n","            rank = xm.get_ordinal(),\n","            shuffle = True\n","        )\n","        dev_sampler = torch.utils.data.distributed.DistributedSampler(\n","            dev_dataset,\n","            num_replicas = xm.xrt_world_size(),\n","            rank = xm.get_ordinal(),\n","            shuffle = False\n","        )\n","        train_dataloader = DataLoader(\n","            train_dataset, \n","            sampler=train_sampler, \n","            batch_size = args.train_batch_size)\n","        \n","        dev_dataloader = DataLoader(\n","            dev_dataset, \n","            sampler = dev_sampler, \n","            batch_size = args.eval_batch_size)\n","    else:\n","        # else we use gpu, colab only provide one gpu, so we won't use distributed trainging\n","        device = torch.device('cuda:0')\n","        train_sampler = RandomSampler(train_dataset) \n","        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n","\n","        dev_sampler = SequentialSampler(dev_dataset) \n","        dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=args.eval_batch_size)\n","        device_num = 1\n","        \n","    train_step = len(train_dataloader)\n","    t_total = train_step // args.gradient_accumulation_steps * args.num_train_epochs // device_num\n","    optimizer = None\n","    scheduler = None\n","    def train_loop_fn(model,loader,device,context):\n","        nonlocal t_total,train_step,device_num \n","        # t_total = len(loader) * args.num_train_epochs\n","        if not args.tpu :\n","            nonlocal optimizer, scheduler\n","            # don't need to init optimizer every epoch if not using tpu\n","            if not optimizer:\n","                no_decay = [\"bias\", \"LayerNorm.weight\"]\n","                optimizer_grouped_parameters = [\n","                    {\n","                        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                        \"weight_decay\": args.weight_decay,\n","                    },\n","                    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","                ]\n","                optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","                scheduler = get_linear_schedule_with_warmup(\n","                    optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","                )\n","                if args.fp16:\n","                    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") \n","\n","        model.zero_grad()\n","        tr_loss = 0.0\n","        iterator = tqdm(enumerate(loader),total = train_step/device_num)\n","        # iterator = enumerate(loader)\n","        for step,batch in iterator:\n","            model.train()\n","            batch = tuple(t.to(device) for t in batch)\n","            inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2],\n","                \"labels\": batch[3]\n","            }\n","            outputs = model(**inputs)\n","            loss = outputs[0]\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","            if args.fp16 and not args.tpu:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","                if args.tpu:\n","                    xm.optimizer_step(optimizer)\n","                else:\n","                    optimizer.step()\n","                scheduler.step() \n","                model.zero_grad()\n","            # check average training loss\n","            if (step + 1)% args.check_loss_step == 0:\n","                avg_loss = (tr_loss/(step+1)) * args.gradient_accumulation_steps\n","                logger.info(\"device:[%s] average_step_loss=%s @ step = %s on epoch = %s\",device,str(avg_loss),str(step+1),str(epoch+1))\n","    \n","    def test_loop_fn(model,loader,device,context):\n","        model.eval()\n","        torch.cuda.empty_cache()\n","        # logger.info(\"Evaluate on {}\".format(set_name))\n","        correct_count = 0\n","        predictions = []\n","        total_test_items = 0\n","        with torch.no_grad():\n","            # iterator = tqdm(enumerate(loader))\n","            for step,batch in enumerate(loader):\n","                model.eval()\n","                batch = tuple(t.to(device) for t in batch)\n","                inputs = {\n","                    \"input_ids\": batch[0],\n","                    \"attention_mask\": batch[1],\n","                    \"token_type_ids\": batch[2],\n","                    \"labels\": batch[3]\n","                }\n","                outputs = model(**inputs)\n","                logits = outputs[1]\n","                prediction = torch.argmax(logits,axis = 1)\n","                correct_count += (prediction == inputs[\"labels\"]).sum().float()\n","                predictions += prediction.cpu().numpy().tolist()\n","                total_test_items += batch[0].shape[0]\n","        logger.info(\"test_items of device[{}] is {}\".format(device,str(total_test_items)))\n","        return correct_count,predictions\n","\n","    def init_status():\n","        ''' \n","        set up status for convenient viewing training result\n","        '''\n","        return {\n","            \"current_epoch\" : 0,\n","            \"best_epoch\" : -1,\n","            \"best_Acc\" : 0.0\n","        }\n","    \n","    status = init_status()\n","    model = select_model(args)\n","    if args.tpu:\n","        model_parallel = dp.DataParallel(model, device_ids=devices)\n","    else:\n","        device = torch.device('cuda:0')\n","        model = model.to(device)\n","\n","    for epoch in range(0,args.num_train_epochs):\n","        logger.info(\"Epoch: {}\".format(str(epoch)))\n","        if args.tpu:\n","            model_parallel(train_loop_fn, train_dataloader)\n","            results = model_parallel(test_loop_fn, dev_dataloader)\n","            correct_count = sum([float(item[0]) for item in results])\n","            predictions = [i for item in results for i in item[1]]\n","            model = model_parallel.models[0]\n","            acc = correct_count / len(dev_examples)\n","        else:\n","            train_loop_fn(model,train_dataloader,device,None)\n","            correct_count, predictions = test_loop_fn(model,dev_dataloader,device,None)\n","            acc = correct_count / len(dev_examples)\n","            acc = acc.cpu().item() # tpu result don't need to switch device \n","        # save model, save status \n","        logger.info(\"DEV ACC : {}% on Epoch {}\".format(str(acc * 100),str(epoch)))\n","        if args.save_method == \"Best_Current\":\n","            if acc > status[\"best_Acc\"]:\n","                status['best_Acc'] = acc\n","                status['best_epoch'] = epoch\n","                model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model itself\n","                best_model_dir = os.path.join(output_dir,\"best_model\")\n","                if not os.path.exists(best_model_dir):\n","                    os.makedirs(best_model_dir)\n","                model_to_save.save_pretrained(best_model_dir)\n","                logger.info(\"best epoch %d has been saved to %s\",epoch,best_model_dir)\n","            model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model itself\n","            current_model_dir = os.path.join(output_dir,\"current_model\")\n","            if not os.path.exists(current_model_dir):\n","                os.makedirs(current_model_dir)\n","            model_to_save.save_pretrained(current_model_dir)\n","            logger.info(\"epoch %d has been saved to %s\",epoch,current_model_dir)\n","        status_dir = os.path.join(output_dir,\"status.json\")\n","        json.dump(status,open(status_dir,'w',encoding = 'utf8'))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"5GneSm9V9JVA","executionInfo":{"status":"ok","timestamp":1604321016955,"user_tz":-480,"elapsed":231199,"user":{"displayName":"快刀切草莓君","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVI52hY-Keqz35442eBgkijOY1l24OWmJ_8DTf=s64","userId":"16350139520541830195"}}},"source":["def make_predictions(args,examples,predictions,omcs_corpus,data_type=\"dev\"):\n","  cs_file = \"OMCS/{}_{}_omcs_of_dataset.json\".format(data_type,args.cs_mode)\n","  with open(cs_file,'r',encoding=\"utf8\") as f:\n","      cs_data = json.load(f)\n","  pred_index = 0 #because it's sequential, we simply index it with examples\n","  result_json = {}\n","  for example in tqdm(examples,desc=\"puting result into examples\"):\n","      result_json[example.id] = {}\n","      result_json[example.id]['question'] = example.question\n","      # result_json[example.id]['endings'] = []\n","      result_json[example.id]['prediction'] = predictions[pred_index]\n","      result_json[example.id][\"prediction_answer\"] = example.endings[predictions[pred_index]]\n","      pred_index += 1\n","      result_json[example.id]['label'] = example.label\n","      example_cs = cs_data[example.id]\n","      result_json[example.id]['endings'] = example_cs['endings']\n","      for ending in result_json[example.id]['endings']:\n","          if args.cs_save_mode == 'id':\n","              ending[\"cs\"] = [omcs_corpus[int(id)] for id in ending[\"cs\"][:args.cs_len]]\n","          else:\n","              ending['cs'] = ending[\"cs\"][:args.cs_len]\n","  return result_json\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9goKVWZ9KuW","executionInfo":{"status":"ok","timestamp":1604321016955,"user_tz":-480,"elapsed":231197,"user":{"displayName":"快刀切草莓君","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVI52hY-Keqz35442eBgkijOY1l24OWmJ_8DTf=s64","userId":"16350139520541830195"}}},"source":["def eval(args,model,dataloader,set_name,device,num_examples):\n","    torch.cuda.empty_cache()\n","    logger.info(\"Evaluating on {}\".format(set_name))\n","    iterator = tqdm(dataloader, desc=\"Iteration\")\n","    correct_count = 0\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for step,batch in enumerate(iterator):\n","            model.eval()\n","            batch = tuple(t.to(device) for t in batch)\n","            inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2],\n","                \"labels\": batch[3]\n","            }\n","            outputs = model(**inputs)\n","            logits = outputs[1]\n","            prediction = torch.argmax(logits,axis = 1)\n","            \n","            correct_count += (prediction == inputs[\"labels\"]).sum().float()\n","            predictions += prediction.cpu().numpy().tolist()\n","    return correct_count/num_examples, predictions"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GAWxC5L8DEC","executionInfo":{"status":"ok","timestamp":1604321016956,"user_tz":-480,"elapsed":231196,"user":{"displayName":"快刀切草莓君","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVI52hY-Keqz35442eBgkijOY1l24OWmJ_8DTf=s64","userId":"16350139520541830195"}}},"source":["logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',datefmt = '%m/%d/%Y %H:%M:%S',level = logging.INFO)\n","logger = logging.getLogger(__name__)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFdTWhq09NII","outputId":"b0eb75e6-de58-43ff-f713-00bcdd47065c","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c386c2f3f93841caacd12588a79289ed","111c874a80ff436b91e5f7d407eee71d","fe77121a5d754c96b684bce72ddbe978","e10b28cbada84cb78cd69c3f6e684971","9459486290704ce6911d9719e9c03c00","039e0d579e2f487c8160300824d409c5","d9fc43dc8c48495eb07c8cf50e1b1c88","51f6365469dd44049671160553969111","996676f15ee047ba8d300bb7f713393f","008e58e33bb34ac5b4dbbb3850fd2b92","ce4ad924656a4cf9b3c266bc769cbd18","edee8d4014c4435fa29da329cb30675c","14a0dfa7c7924189a1ce86bb9a92b1d3","251c40a0378d431b8ee8c0d49689e258","aad72d8e03f14d8eb1fbe8293077f2bf","31ce00aa44394bff8304aef7304b3189"]}},"source":["parser = argparse.ArgumentParser()\n","# data arguments\n","parser.add_argument(\"--data_dir\",type = str,default = \"dataset/CSQA\")\n","parser.add_argument(\"--cs_dir\",type = str, default = \"OMCS\")\n","parser.add_argument(\"--test_file\",type= str,default = \"test_rand_split_no_answers.jsonl\")\n","parser.add_argument(\"--dev_file\",type= str,default = \"dev_rand_split.jsonl\")\n","parser.add_argument(\"--train_file\",type = str,default = \"train_rand_split.jsonl\")\n","parser.add_argument(\"--output_dir\",type = str,default = \"model\")\n","parser.add_argument(\"--save_model_name\",type = str,default = \"bert_csqa_2e-5_wholeQA-Match_cslen5\")\n","parser.add_argument(\"--tokenizer_name_or_path\",type = str,default = \"bert-base-cased\")\n","parser.add_argument(\"--origin_model\",type = str,default = \"bert-base-cased\", help = \"origin model dir for training\")\n","parser.add_argument(\"--omcs_file\",type=str,default = \"omcs-free-origin.json\")\n","# hyper parameters\n","parser.add_argument(\"--max_length\",type=int,default = 80 )\n","parser.add_argument(\"--gradient_accumulation_steps\",type=int,default=1,help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n","parser.add_argument(\"--num_train_epochs\",default=5,type=int)\n","parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n","parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","parser.add_argument(\"--learning_rate\", default=2e-5, type=float, help=\"The initial learning rate for Adam.\")\n","parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n","parser.add_argument(\"--train_batch_size\", default=15, type=int, help=\"Batch size for training.\")\n","parser.add_argument(\"--eval_batch_size\", default=6, type=int, help=\"Batch size for eval.\")\n","parser.add_argument(\"--check_loss_step\",default = 400,type = int,help = \"output current average loss of training\")\n","parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","parser.add_argument(\"--cs_len\",type = int, default = 5)\n","# settings\n","parser.add_argument(\"--n_gpu\",type=int , default = 1)\n","parser.add_argument(\"--fp16\",action = \"store_true\")\n","parser.add_argument(\"--save_method\",type = str,default = \"Best_Current\")\n","parser.add_argument(\"--do_finetune\",action = \"store_true\",default = False)\n","# parser.add_argument(\"--cs_mode\",type = str,default = \"wholeQA-Match\")\n","parser.add_argument(\"--cs_mode\",type = str,default = \"QAconcept-Match\")\n","parser.add_argument(\"--cs_save_mode\",type = str,default = \"id\")\n","parser.add_argument(\"--seed\",type = int,default = None,help = \"freeze seed\")\n","parser.add_argument('--tpu',action = \"store_true\")\n","parser.add_argument('--task_name',type = str, default = \"baseline\")\n","args = parser.parse_args([])\n","# if args.tpu:\n","#     tpu_training(args)\n","# else:\n","train(args)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["11/02/2020 12:53:21 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"],"name":"stderr"},{"output_type":"stream","text":["model/feature_cache/cached_train_QAconcept-Match_baseline_5\n"],"name":"stdout"},{"output_type":"stream","text":["puting commonsencs into examples: 100%|██████████| 9741/9741 [00:00<00:00, 53116.41it/s]\n","CSQA processing: 100%|██████████| 9741/9741 [00:50<00:00, 194.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["model/feature_cache/cached_dev_QAconcept-Match_baseline_5\n"],"name":"stdout"},{"output_type":"stream","text":["puting commonsencs into examples: 100%|██████████| 1221/1221 [00:00<00:00, 41415.59it/s]\n","CSQA processing: 100%|██████████| 1221/1221 [00:06<00:00, 201.30it/s]\n","11/02/2020 12:54:25 - INFO - filelock -   Lock 140562873029520 acquired on model/cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock\n","11/02/2020 12:54:25 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /content/drive/My Drive/rerank_CSQA/model/cache/tmph2cjt4rm\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c386c2f3f93841caacd12588a79289ed","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["11/02/2020 12:54:25 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json in cache at model/cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n","11/02/2020 12:54:25 - INFO - transformers.file_utils -   creating metadata file for model/cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n","11/02/2020 12:54:25 - INFO - filelock -   Lock 140562873029520 released on model/cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock\n","11/02/2020 12:54:25 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at model/cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n","11/02/2020 12:54:25 - INFO - transformers.configuration_utils -   Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 28996\n","}\n","\n","11/02/2020 12:54:25 - INFO - filelock -   Lock 140562887679560 acquired on model/cache/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\n","11/02/2020 12:54:25 - INFO - transformers.file_utils -   https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /content/drive/My Drive/rerank_CSQA/model/cache/tmpn15jkioc\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"996676f15ee047ba8d300bb7f713393f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["11/02/2020 12:54:32 - INFO - transformers.file_utils -   storing https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin in cache at model/cache/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n","11/02/2020 12:54:32 - INFO - transformers.file_utils -   creating metadata file for model/cache/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n","11/02/2020 12:54:32 - INFO - filelock -   Lock 140562887679560 released on model/cache/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\n","11/02/2020 12:54:32 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at model/cache/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["11/02/2020 12:54:36 - INFO - transformers.modeling_utils -   Weights of BertForMultipleChoice not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","11/02/2020 12:54:36 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","11/02/2020 12:54:55 - INFO - __main__ -   Epoch: 0\n"," 61%|██████▏   | 399/650.0 [12:01<07:38,  1.83s/it]11/02/2020 13:06:58 - INFO - __main__ -   device:[cuda:0] average_step_loss=1.5933694750070573 @ step = 400 on epoch = 1\n","100%|██████████| 650/650.0 [19:38<00:00,  1.81s/it]\n","11/02/2020 13:15:27 - INFO - __main__ -   test_items of device[cuda:0] is 1221\n","11/02/2020 13:15:27 - INFO - __main__ -   DEV ACC : 48.81244897842407% on Epoch 0\n","11/02/2020 13:15:27 - INFO - transformers.configuration_utils -   Configuration saved in model/bert_csqa_2e-5_wholeQA-Match_cslen5/best_model/config.json\n","11/02/2020 13:15:29 - INFO - transformers.modeling_utils -   Model weights saved in model/bert_csqa_2e-5_wholeQA-Match_cslen5/best_model/pytorch_model.bin\n","11/02/2020 13:15:29 - INFO - __main__ -   best epoch 0 has been saved to model/bert_csqa_2e-5_wholeQA-Match_cslen5/best_model\n","11/02/2020 13:15:29 - INFO - transformers.configuration_utils -   Configuration saved in model/bert_csqa_2e-5_wholeQA-Match_cslen5/current_model/config.json\n","11/02/2020 13:15:31 - INFO - transformers.modeling_utils -   Model weights saved in model/bert_csqa_2e-5_wholeQA-Match_cslen5/current_model/pytorch_model.bin\n","11/02/2020 13:15:31 - INFO - __main__ -   epoch 0 has been saved to model/bert_csqa_2e-5_wholeQA-Match_cslen5/current_model\n","11/02/2020 13:15:31 - INFO - __main__ -   Epoch: 1\n"," 58%|█████▊    | 377/650.0 [11:28<08:17,  1.82s/it]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hlC6Zqe9K_YL","executionInfo":{"status":"aborted","timestamp":1604321023759,"user_tz":-480,"elapsed":237993,"user":{"displayName":"快刀切草莓君","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVI52hY-Keqz35442eBgkijOY1l24OWmJ_8DTf=s64","userId":"16350139520541830195"}}},"source":[""],"execution_count":null,"outputs":[]}]}