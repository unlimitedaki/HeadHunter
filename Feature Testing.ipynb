{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel,BertTokenizer,AlbertTokenizer,RobertaTokenizer,XLNetTokenizer\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "from transformers.modeling_utils import SequenceSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "# data arguments\n",
    "parser.add_argument(\"--data_dir\",type = str,default = \"dataset/CSQA\")\n",
    "parser.add_argument(\"--cs_dir\",type = str, default = \"OMCS\")\n",
    "parser.add_argument(\"--test_file\",type= str,default = \"test_rand_split_no_answers.jsonl\")\n",
    "parser.add_argument(\"--dev_file\",type= str,default = \"dev_rand_split.jsonl\")\n",
    "parser.add_argument(\"--train_file\",type = str,default = \"train_rand_split.jsonl\")\n",
    "parser.add_argument(\"--output_dir\",type = str,default = \"model\")\n",
    "parser.add_argument(\"--save_model_name\",type = str,default = \"bert_csqa_2e-5_wholeQA-Match_cslen5\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\",type = str,default = \"bert-base-cased\")\n",
    "parser.add_argument(\"--origin_model\",type = str,default = \"bert-base-cased\", help = \"origin model dir for training\")\n",
    "parser.add_argument(\"--omcs_file\",type=str,default = \"omcs-free-origin.json\")\n",
    "# hyper parameters\n",
    "parser.add_argument(\"--max_length\",type=int,default = 80 )\n",
    "parser.add_argument(\"--gradient_accumulation_steps\",type=int,default=1,help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument(\"--num_train_epochs\",default=5,type=int)\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--learning_rate\", default=2e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "parser.add_argument(\"--train_batch_size\", default=15, type=int, help=\"Batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\", default=6, type=int, help=\"Batch size for eval.\")\n",
    "parser.add_argument(\"--check_loss_step\",default = 400,type = int,help = \"output current average loss of training\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--cs_len\",type = int, default = 5)\n",
    "# settings\n",
    "parser.add_argument(\"--n_gpu\",type=int , default = 1)\n",
    "parser.add_argument(\"--fp16\",action = \"store_true\")\n",
    "parser.add_argument(\"--save_method\",type = str,default = \"Best_Current\")\n",
    "parser.add_argument(\"--do_finetune\",action = \"store_true\",default = False)\n",
    "parser.add_argument(\"--cs_mode\",type = str,default = \"QAconcept-Match\")\n",
    "parser.add_argument(\"--cs_save_mode\",type = str,default = \"id\")\n",
    "parser.add_argument(\"--seed\",type = int,default = None,help = \"freeze seed\")\n",
    "parser.add_argument('--tpu',action = \"store_true\")\n",
    "parser.add_argument('--task_name',type = str, default = \"baseline\")\n",
    "parser.add_argument(\"--test\",action = \"store_true\")\n",
    "parser.add_argument(\"--dev\",action = \"store_true\")\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_dir': 'dataset/CSQA',\n",
       " 'cs_dir': 'OMCS',\n",
       " 'test_file': 'test_rand_split_no_answers.jsonl',\n",
       " 'dev_file': 'dev_rand_split.jsonl',\n",
       " 'train_file': 'train_rand_split.jsonl',\n",
       " 'output_dir': 'model',\n",
       " 'save_model_name': 'bert_csqa_2e-5_wholeQA-Match_cslen5',\n",
       " 'tokenizer_name_or_path': 'bert-base-cased',\n",
       " 'origin_model': 'bert-base-cased',\n",
       " 'omcs_file': 'omcs-free-origin.json',\n",
       " 'max_length': 80,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'num_train_epochs': 5,\n",
       " 'weight_decay': 0.0,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'learning_rate': 2e-05,\n",
       " 'warmup_steps': 0,\n",
       " 'train_batch_size': 15,\n",
       " 'eval_batch_size': 6,\n",
       " 'check_loss_step': 400,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'cs_len': 5,\n",
       " 'n_gpu': 1,\n",
       " 'fp16': False,\n",
       " 'save_method': 'Best_Current',\n",
       " 'do_finetune': False,\n",
       " 'cs_mode': 'wholeQA-Match',\n",
       " 'cs_save_mode': 'id',\n",
       " 'seed': None,\n",
       " 'tpu': False,\n",
       " 'task_name': 'baseline',\n",
       " 'test': False,\n",
       " 'dev': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "omcs_corpus = load_omcs(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(omcs_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tokenizer(args):\n",
    "    if \"albert\" in args.origin_model:\n",
    "        return AlbertTokenizer.from_pretrained(args.origin_model)\n",
    "    elif \"roberta\" in args.origin_model:\n",
    "        return RobertaTokenizer.from_pretrained(args.origin_model)\n",
    "    elif \"bert\" in args.origin_model:\n",
    "        return BertTokenizer.from_pretrained(args.origin_model)\n",
    "    elif \"xlnet\" in args.origin_model:\n",
    "        return XLNetTokenizer.from_pretrained(args.origin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(args,model_name = None):\n",
    "    if not model_name:\n",
    "        model_name = args.origin_model\n",
    "        cache = os.path.join(args.output_dir,\"cache\")\n",
    "    else:\n",
    "        cache = model_name\n",
    "    if args.task_name == \"rerank_csqa\":\n",
    "        if \"albert\" in model_name:\n",
    "            return AlbertAttRanker.from_pretrained(model_name,cache_dir = cache,cs_len = args.cs_len)\n",
    "        elif \"roberta\" in model_name:\n",
    "            return RobertaAttRanker.from_pretrained(model_name,cache_dir = cache,cs_len = args.cs_len)\n",
    "        elif \"bert\" in model_name:\n",
    "            return BertAttRanker.from_pretrained(model_name,cache_dir = cache,cs_len = args.cs_len)\n",
    "        elif \"xlnet\" in model_name:\n",
    "            return XLNetAttRanker.from_pretrained(model_name,cache_dir = cache,cs_len = args.cs_len)\n",
    "    elif args.task_name == \"rerank_csqa_without_rerank\":\n",
    "        if \"bert\" in model_name:\n",
    "            return BertAttRankerDontRank.from_pretrained(model_name,cache_dir = cache,cs_len = args.cs_len)\n",
    "    else:\n",
    "        if \"albert\" in model_name:\n",
    "            return AlbertForMultipleChoice.from_pretrained(model_name,cache_dir = cache)\n",
    "        elif \"roberta\" in model_name:\n",
    "            return RobertaForMultipleChoice.from_pretrained(model_name,cache_dir = cache)\n",
    "        elif \"bert\" in model_name:\n",
    "            return BertForMultipleChoice.from_pretrained(model_name,cache_dir = cache)\n",
    "        elif \"xlnet\" in model_name:\n",
    "            return XLNetForMultipleChoice.from_pretrained(model_name,cache_dir = cache)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = select_tokenizer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\\feature_cache\\cached_train_QAconcept-Match_baseline_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "puting commonsencs into examples: 100%|█████████████████████████████████████████| 9741/9741 [00:00<00:00, 30248.95it/s]\n",
      "CSQA processing: 100%|████████████████████████████████████████████████████████████| 9741/9741 [00:39<00:00, 247.16it/s]\n"
     ]
    }
   ],
   "source": [
    "_,_,train_dataset= load_csqa_omcs_dataset(tokenizer,args,omcs_corpus,\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/CSQA\\\\train_rand_split.jsonl'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = os.path.join(args.data_dir,args.train_file) \n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([0,1,2,1], dtype=torch.int64), num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.arange(1,4, dtype=torch.long).reshape(3, 1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_labels = F.one_hot(labels, num_classes=5) # [3,1] -> [3,1,5]\n",
    "# oh_labels\n",
    "oh_labels = oh_labels.squeeze(1)  # [3,1,5] -> [3,5]\n",
    "oh_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5587,  0.8788,  0.4759, -0.3196, -0.5946],\n",
       "        [ 0.1233, -0.5129,  0.8072,  0.4943,  0.3895],\n",
       "        [ 0.2659, -0.5676,  0.6588, -1.4019, -0.3201]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_logits = torch.randn(3,5)\n",
    "reshaped_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor([[False,  True, False, False, False],\n",
      "        [False, False,  True, False, False],\n",
      "        [False, False, False,  True, False]])\n",
      "tensor([[0.0225],\n",
      "        [0.1821],\n",
      "        [0.0409]])\n",
      "tensor([[0.0225],\n",
      "        [0.1821],\n",
      "        [0.0409]])\n"
     ]
    }
   ],
   "source": [
    "# 过 softmax\n",
    "reshaped_logits = F.softmax(reshaped_logits, dim=1)\n",
    "print(reshaped_logits)\n",
    "# mask 负例\n",
    "mask = oh_labels.bool()\n",
    "print(mask)\n",
    "# 仅留下正例\n",
    "logits_p = reshaped_logits.masked_select(mask).reshape(-1, 1)\n",
    "print(logits_pr)\n",
    "# 重复\n",
    "logits_p.repeat(1, 5)\n",
    "print(logits_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0382, 0.4373, 0.2923, 0.1319, 0.1002],\n",
       "        [0.1596, 0.0845, 0.3163, 0.2313, 0.2083],\n",
       "        [0.2732, 0.1187, 0.4046, 0.0515, 0.1520]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_n = reshaped_logits\n",
    "logits_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3991,  0.0000,  0.1450,  0.3054,  0.3371],\n",
       "        [ 0.1567,  0.2318,  0.0000,  0.0850,  0.1080],\n",
       "        [-0.2216, -0.0672, -0.3531,  0.0000, -0.1005]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_np = logits_p - logits_n\n",
    "logits_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = nn.HingeEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1,  1, -1, -1, -1],\n",
       "        [-1, -1,  1, -1, -1],\n",
       "        [-1, -1, -1,  1, -1]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_oh2 = oh_labels * 2 -1\n",
    "labels_oh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7316)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loss(logits_np, labels_oh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HingeEmbeddingLoss in module torch.nn.modules.loss:\n",
      "\n",
      "class HingeEmbeddingLoss(_Loss)\n",
      " |  HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')\n",
      " |  \n",
      " |  Measures the loss given an input tensor :math:`x` and a labels tensor :math:`y`\n",
      " |  (containing 1 or -1).\n",
      " |  This is usually used for measuring whether two inputs are similar or\n",
      " |  dissimilar, e.g. using the L1 pairwise distance as :math:`x`, and is typically\n",
      " |  used for learning nonlinear embeddings or semi-supervised learning.\n",
      " |  \n",
      " |  The loss function for :math:`n`-th sample in the mini-batch is\n",
      " |  \n",
      " |  .. math::\n",
      " |      l_n = \\begin{cases}\n",
      " |          x_n, & \\text{if}\\; y_n = 1,\\\\\n",
      " |          \\max \\{0, \\Delta - x_n\\}, & \\text{if}\\; y_n = -1,\n",
      " |      \\end{cases}\n",
      " |  \n",
      " |  and the total loss functions is\n",
      " |  \n",
      " |  .. math::\n",
      " |      \\ell(x, y) = \\begin{cases}\n",
      " |          \\operatorname{mean}(L), & \\text{if reduction} = \\text{'mean';}\\\\\n",
      " |          \\operatorname{sum}(L),  & \\text{if reduction} = \\text{'sum'.}\n",
      " |      \\end{cases}\n",
      " |  \n",
      " |  where :math:`L = \\{l_1,\\dots,l_N\\}^\\top`.\n",
      " |  \n",
      " |  Args:\n",
      " |      margin (float, optional): Has a default value of `1`.\n",
      " |      size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      " |          the losses are averaged over each loss element in the batch. Note that for\n",
      " |          some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
      " |          is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      " |          when reduce is ``False``. Default: ``True``\n",
      " |      reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      " |          losses are averaged or summed over observations for each minibatch depending\n",
      " |          on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      " |          batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      " |      reduction (string, optional): Specifies the reduction to apply to the output:\n",
      " |          ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      " |          ``'mean'``: the sum of the output will be divided by the number of\n",
      " |          elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      " |          and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      " |          specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(*)` where :math:`*` means, any number of dimensions. The sum operation\n",
      " |        operates over all the elements.\n",
      " |      - Target: :math:`(*)`, same shape as the input\n",
      " |      - Output: scalar. If :attr:`reduction` is ``'none'``, then same shape as the input\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      HingeEmbeddingLoss\n",
      " |      _Loss\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, margin=1.0, size_average=None, reduce=None, reduction='mean')\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input, target)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __constants__ = ['margin', 'reduction']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self)\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse=True)\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should reimplement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse=True)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |      \n",
      " |      .. warning ::\n",
      " |      \n",
      " |          The current implementation will not have the presented behavior\n",
      " |          for complex :class:`Module` that perform many operations.\n",
      " |          In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only\n",
      " |          contain the gradients for a subset of the inputs and outputs.\n",
      " |          For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`\n",
      " |          directly on a specific input or output to get the required gradients.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  requires_grad_(self, requires_grad=True)\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point desired :attr:`dtype` s. In addition, this method will\n",
      " |      only cast the floating point parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
      " |              the floating point parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.HingeEmbeddingLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_logits  # batch_size, choice_num\n",
    "labels       # btach_size, 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kernel",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
